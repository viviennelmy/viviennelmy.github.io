{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0b61aa-1367-4c61-b976-ae02c7011562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vivienne/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Model Topics:\n",
      "Topic 1:\n",
      "communication worst miami stand mins passengers experienced club service aviv\n",
      "Topic 2:\n",
      "class service food business london seat crew staff seats cabin\n",
      "Topic 3:\n",
      "pandemic assignments diverted rotterdam releases 19th roof minor parents grandfather\n",
      "Topic 4:\n",
      "service london hours seat told time business class staff customer\n",
      "Topic 5:\n",
      "seat staff seats class delayed london business luggage required arrogant\n",
      "\n",
      "NMF Model Topics:\n",
      "Topic 1:\n",
      "crew food cabin service meal water aircraft london staff poor\n",
      "Topic 2:\n",
      "class business first lounge seats economy worst middle seat never\n",
      "Topic 3:\n",
      "refund customer cancelled call booked service voucher email phone told\n",
      "Topic 4:\n",
      "luggage delayed hours staff airport check boarding minutes arrived hour\n",
      "Topic 5:\n",
      "seat seats economy extra premium airlines legroom paid front aisle\n",
      "[--------------------------------------------------] 1.2% 20.1/1662.8MB downloaded"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ensure that the required NLTK data is downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('filtered_negreviews/filtered_negreviews.csv')\n",
    "\n",
    "# Define a function for preprocessing text\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    \n",
    "    # Tokenize the text by splitting on whitespace\n",
    "    words = text.split()\n",
    "    \n",
    "    # Filter out words with 3 characters or less\n",
    "    filtered_words = [word for word in words if len(word) > 3]\n",
    "    \n",
    "    # Join the filtered words back into a single string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply preprocessing to the 'ReviewBody' column\n",
    "df['cleaned_review'] = df['ReviewBody'].apply(preprocess_text)\n",
    "\n",
    "# Define domain-specific stopwords\n",
    "domain_stopwords = ['british', 'airway', 'airways', 'flights', 'ba', 'plane', \n",
    "                     'flight', 'ife', 'yo', 'ba2058', 'ba169', 'us']\n",
    "\n",
    "# Combine NLTK stopwords with domain-specific stopwords\n",
    "stop_words = list(set(stopwords.words('english')).union(domain_stopwords))\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fit and transform the cleaned reviews\n",
    "X = vectorizer.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Set the number of topics\n",
    "n_topics = 5\n",
    "\n",
    "# Apply LDA model\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_topics = lda_model.fit_transform(X)\n",
    "\n",
    "# Apply NMF model\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "nmf_topics = nmf_model.fit_transform(X)\n",
    "\n",
    "# Function to display the top words for each topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "\n",
    "print(\"LDA Model Topics:\")\n",
    "display_topics(lda_model, vectorizer.get_feature_names_out(), no_top_words)\n",
    "\n",
    "print(\"\\nNMF Model Topics:\")\n",
    "display_topics(nmf_model, vectorizer.get_feature_names_out(), no_top_words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "44900fff-9a11-4312-b965-152df53ea607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Model Coherence Score: 0.4080976120755132\n",
      "NMF Model Coherence Score: 0.4413820044520914\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('filtered_negreviews/filtered_negreviews.csv')\n",
    "\n",
    "# Define a function for preprocessing text\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    # Tokenize the text by splitting on whitespace\n",
    "    words = text.split()\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to the 'ReviewBody' column\n",
    "df['cleaned_review'] = df['ReviewBody'].apply(preprocess_text)\n",
    "\n",
    "# Preprocess the text data\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['cleaned_review'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a Gensim Dictionary and Corpus\n",
    "texts = [doc.split() for doc in df['cleaned_review']]  # Tokenize the documents\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "def calculate_coherence(model, X, feature_names, texts, dictionary):\n",
    "    topics_words = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
    "        topics_words.append(top_words)\n",
    "    coherence_model = CoherenceModel(topics=topics_words, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score\n",
    "\n",
    "# LDA model Coherence\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_model.fit(X)\n",
    "lda_coherence = calculate_coherence(lda_model, X, feature_names, texts, dictionary)\n",
    "print(f\"LDA Model Coherence Score: {lda_coherence}\")\n",
    "\n",
    "# NMF model Coherence\n",
    "nmf_model = NMF(n_components=5, random_state=42)\n",
    "nmf_model.fit(X)\n",
    "nmf_coherence = calculate_coherence(nmf_model, X, feature_names, texts, dictionary)\n",
    "print(f\"NMF Model Coherence Score: {nmf_coherence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('filtered_negreviews/cleaned_reviews.csv')\n",
    "median_value = df['SeatComfort'].median()\n",
    "df['SeatComfort'].fillna(median_value, inplace=True)\n",
    "# Define a function for preprocessing text\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    # Tokenize the text by splitting on whitespace\n",
    "    words = text.split()\n",
    "    # Filter out words with 3 characters or less\n",
    "    filtered_words = [word for word in words if len(word) > 3]\n",
    "    # Join the filtered words back into a single string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply preprocessing to the 'ReviewBody' column\n",
    "df['cleaned_review'] = df['ReviewBody'].apply(preprocess_text)\n",
    "\n",
    "# Define domain-specific stopwords\n",
    "domain_stopwords = ['british', 'airway', 'airways', 'flights', 'ba', 'plane', \n",
    "                     'flight', 'ife', 'yo', 'ba2058', 'ba169', 'us']\n",
    "\n",
    "# Combine NLTK stopwords with domain-specific stopwords\n",
    "stop_words = list(set(stopwords.words('english')).union(domain_stopwords))\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fit and transform the cleaned reviews\n",
    "X = vectorizer.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Convert the sparse matrix to a dense matrix and then to an array\n",
    "X_dense = X.toarray()\n",
    "\n",
    "# Target variable\n",
    "y = df['SeatComfort']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dense, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Mean Squared Error: {mean_squared_error(y_test, y_pred)}')\n",
    "\n",
    "x_feature = X_dense[:, 0]  \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_feature, y, color='blue', label='Data Points')\n",
    "plt.plot(X_test[:, 0], y_pred, color='red', linewidth=2, label='Regression Line')\n",
    "\n",
    "plt.xlabel('Feature 1 (TF-IDF)')\n",
    "plt.ylabel('Seat Comfort')\n",
    "plt.title('Regression of Seat Comfort on Feature 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8246e476-40c3-4f00-be77-51dbbdddb8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in /opt/anaconda3/lib/python3.11/site-packages (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.24.2 in /opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.24.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.10.1)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (2.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (3.1.3)\n",
      "Requirement already satisfied: numexpr in /opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (2.8.7)\n",
      "Requirement already satisfied: funcy in /opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.5.1)\n",
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (4.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (68.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from gensim->pyLDAvis) (2.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->pyLDAvis) (2.1.3)\n",
      "Requirement already satisfied: pyfume in /opt/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim->pyLDAvis) (0.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: simpful in /opt/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (2.12.0)\n",
      "Requirement already satisfied: fst-pso in /opt/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (4.9.0)\n",
      "Requirement already satisfied: miniful in /opt/anaconda3/lib/python3.11/site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (0.0.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3e898-c0de-45bf-a7ed-d2a557ffe904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the corpus and dictionary for Gensim\n",
    "texts = df['cleaned_review'].map(tokenize).tolist()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus_gensim = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Apply LDA model using Gensim\n",
    "num_topics = 5\n",
    "lda_model = gensim.models.LdaModel(corpus_gensim, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "\n",
    "# Visualize the topics using pyLDAvis\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus_gensim, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
